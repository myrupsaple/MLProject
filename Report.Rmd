---
title: "Machine Learning Course Project"
author: "Riley Matsuda"
date: "10/19/2019"
output: 
  html_document: 
    keep_md: yes
---

# Preprocessing and Cleaning

```{r, echo = TRUE}
library(caret)
training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")
dim(training); dim(testing)
summary(colSums(is.na(training)))
summary(colSums(is.na(testing)))
```

Initial observations of the datasets indicate a lot of variables with NA values.
We will remove all of the columns containing NA values. Furthermore, some of the
column classes in the training set do not match those in the test set. We will
remove those columns as well, with the exception of the test set's "problem_id"
column. In addition to removing these variables, the variables with near-zero
variation will be removed. The first five columns are labels that would not be 
expected to have any correlation to the data collected, so they will also be
removed.

```{r, echo = TRUE}
colToRemove <- (colSums(is.na(training)) != 0 | colSums(is.na(testing) != 0))
colToRemove <- !(sapply(training, class) == sapply(testing, class))
colToRemove[which(names(testing) == "problem_id")] <- FALSE
colToRemove[nearZeroVar(training)] <- TRUE
colToRemove[1:5] <- TRUE
names_train <- names(training)[!colToRemove]
names_test <- names(testing)[!colToRemove]
training <- training[!colToRemove]
testing <- testing[!colToRemove]
names(training) <- names_train
names(testing) <- names_test
dim(training); dim(testing)
```

The data in both sets has been reduced to 51 dimensions from the original 160.

# Prediction Model Development

Before creating any prediction models, the training data will be subsetted into
training and validation sets.

```{r, echo = TRUE}
inValid <- createDataPartition(y = training$classe, p = 0.2, list = FALSE)
sub_train <- training[-inValid, ]
sub_valid <- training[inValid, ]
```

### Random Forest

The random forest model will be run on the subsetted training data. The number
of trees is limited to 50 to reduce runtime.

```{r, echo = TRUE}
set.seed(2000)
mod_rf <- train(classe ~ ., method = "rf", data = sub_train, ntree = 50,
                trControl = trainControl(method = "cv", number = 3))
mod_rf$finalModel

pred_rf <- predict(mod_rf, sub_valid)
cm_rf <- confusionMatrix(pred_rf, sub_valid$classe)
cm_rf
```

Based on the cross-validation data, the random forest model has an expected out 
of sample errror rate of 0.23%.

### Boosting

The generalized boosted model will be run on the subsetted training data.

```{r, echo = TRUE}
mod_gbm <- train(classe ~ ., method = "gbm", data = sub_train, verbose = FALSE,
                 trControl = trainControl(method = "cv", number = 3))
mod_gbm$finalModel

pred_gbm <- predict(mod_gbm, sub_valid)
cm_gbm <- confusionMatrix(pred_gbm, sub_valid$classe)
cm_gbm
```

The boosted model has an expected out of sample error rate of 1.3%, which is
slightly higher than the random forest model. We will use the random forest
model on our test data.

# Applying Our Model to the Test Set
```{r, echo = TRUE}
pred_final <- predict(mod_rf, testing)
pred_final
```

This output represents our predicted activity classes for the test data.